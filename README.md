<p align="center">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="license" />
  <img src="https://img.shields.io/badge/Built%20with-Python%203.12-blue.svg" alt="python" />
  <img src="https://img.shields.io/badge/Powered%20by-PyTorch%20%7C%20Diffusers%20%7C%20Accelerate-orange.svg" alt="powered by" />
</p>

<p align="center">
  <a href="docs/TFG_Utica_Maxim.pdf" download>
    <img src="https://img.shields.io/badge/ğŸ“˜%20Download%20TFG-TFG_Utica_Maxim.pdf-blue" alt="Download TFG"/>
  </a>
</p>


# Diffusion Models in Decentralized Federated Learning

This repository contains a modular simulation environment for training and evaluating **Denoising Diffusion Probabilistic Models (DDPMs)** in **Decentralized Federated Learning (DFL)** scenarios. The system was developed as part of a Bachelors's Thesis project focused on generative AI and distributed training.

## ğŸ§  Project Overview

**Title**: Implementation of Generative Diffusion Models in Decentralized Federated Learning  
**Author**: Maxim Utica Babyak  
**Degree**: Bachelor's in Computer Engineering  
**University**: Universidad de Murcia â€“ Facultad de InformÃ¡tica  
**Date**: January 2025  
**Language**: Spanish  

This project explores how diffusion models can enhance the performance of decentralized federated learning systems, improving convergence, privacy, and robustness under non-IID data distributions.

You can read the full thesis here:  
ğŸ“˜ [TFG_Utica_Maxim.pdf](docs/TFG_Utica_Maxim.pdf)

## ğŸ“Œ Key Features

* âœ… Simulation of decentralized topologies (e.g., ring, custom)
* ğŸ§© Modular codebase with YAML-configurable experiments
* âŸ³ Decentralized training loop with model aggregation
* ğŸ§  Conditional DDPM generation with U-Net backbone
* ğŸ”’ Label-sharing strategy for non-IID mitigation
* ğŸ“Š Built-in evaluation pipeline using auxiliary classifiers

## ğŸ“‚ Project Structure

```
diffusion-dfl-implementation/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ ring/                     # Training & testing configs for ring topology
â”‚   â””â”€â”€ custom/                   # Training & testing configs for custom topology
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ cli.py                # CLI argument parsing
â”‚   â”‚   â”œâ”€â”€ config.py             # Config dataclasses and YAML loaders
â”‚   â”‚   â”œâ”€â”€ training.py           # Training logic
â”‚   â”‚   â”œâ”€â”€ testing.py            # Evaluation logic
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # DDPM sampling pipeline
â”‚   â”‚   â”œâ”€â”€ launch.py             # Node orchestration and data loading
â”‚   â”‚   â””â”€â”€ filesystem.py         # Utility functions for I/O, timers, etc.
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ classifier.py         # CNN classifier for evaluation
â”‚       â””â”€â”€ filtered_dataset.py   # Dataset wrapper with label/threshold filtering
â”‚
â”œâ”€â”€ laboratory/                   # Workspace for runtime data
â”‚   â”œâ”€â”€ datasets/                 # Raw data downloaded via torchvision
â”‚   â”œâ”€â”€ classifiers/              # Saved classifier weights
â”‚   â”œâ”€â”€ topologies/               # neighbours.yaml and labels-*.yaml per topology
â”‚   â”œâ”€â”€ scenarios/                # Training outputs (per run)
â”‚   â””â”€â”€ evaluations/              # Evaluation outputs (per model)
â”‚
â”œâ”€â”€ run.py                        # Script to run training or testing
â”œâ”€â”€ pyproject.toml                # ğŸ“¦ Dependency and build configuration
â”œâ”€â”€ LICENSE                       # License (MIT)
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ .gitignore                    # Git ignore rules
```

## ğŸ“¦ Requirements

* Python 3.12
* **A CUDA 12.1-compatible NVIDIA GPU** (e.g., RTX 30xx or 40xx series)
* PyTorch with GPU support (cu121)

## ğŸ“¥ Installation

This project uses [PEP 621](https://peps.python.org/pep-0621/) with `pyproject.toml`.

1. **Install PyTorch with CUDA 12.1 support**:

```bash
pip install torch torchvision torchmetrics -i https://download.pytorch.org/whl/cu121
```

2. **Install the project dependencies**:

```bash
pip install .
```

## â–¶ï¸ Running Experiments

All experiments are executed via the `entrypoint.py` module. Specify mode and configuration YAML:

### ğŸ”¹ Training

```bash
python run.py train mnist ring
python run.py test emnist ring --split letters
```

### ğŸ”¹ Evaluation

```bash
python -m src.entrypoint test --config configs/custom/test_emnist_letters.yaml
```

## ğŸ“ˆ Results

Empirical results demonstrate strong convergence and generalization under decentralized training:

| Dataset        | Accuracy (Best) |
| -------------- | --------------- |
| MNIST          | 98.6%           |
| FashionMNIST   | 91.79%          |
| EMNIST Letters | 90.49%          |

These results were achieved on 10-node topologies with label-sharing enabled.

## ğŸ“š Academic Reference

If you use this project in academic work, please cite this repository. A BibTeX entry will be available upon request.

## ğŸ§ª Future Work

* Secure aggregation protocols
* Asynchronous and hierarchical topologies
* Integration of differential privacy
* Real-time federated learning agents

## ğŸ§¾ License

This project is licensed under the MIT License.